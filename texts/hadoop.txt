http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html

$ sudo addgroup hadoop
$ sudo adduser --ingroup hadoop hduser

-- ssh access
$ su - hduser

-- generate an SSH key for the hduser user
$ ssh-keygen -t rsa -P ""

-- enable SSH access to your local machine with this newly created key
$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

-- test
ssh localhost

--hadoop start/restart
/usr/local/hadoop/sbin/start-dfs.sh

--hadoop stop
/usr/local/hadoop/sbin/stop-dfs.sh

--start yarn
/usr/local/hadoop/sbin/start-yarn.sh

--stop yarn
/usr/local/hadoop/sbin/stop-yarn.sh

-- create dir
hadoop fs -mkdir /user
hadoop fs -mkdir /user/hduser
hadoop fs -mkdir /user/hduser/gutenberg

-- copy book to hdfs
hdfs dfs -copyFromLocal /tmp/gutenberg /user/hduser/gutenberg

-- check contents of directory
hdfs dfs -ls /user/hduser/gutenberg

-- remove if exists already
hadoop fs -rm -r /user/hduser/gutenberg-output

-- run example wordcount map reduce job
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /user/hduser/gutenberg/* /user/hduser/gutenberg-output

-- check output was generated
hdfs dfs -ls /user/hduser/gutenberg-output

-- copy output to local directory
hadoop fs -copyToLocal /user/hduser/gutenberg-output/part-r-00000 /home/hduser/projects

-- check hdfs webui 
http://localhost:50070/ – web UI of the NameNode daemon
http://localhost:50030/ – web UI of the JobTracker daemon
http://localhost:50060/ – web UI of the TaskTracker daemon




-- install hive
hadoop fs -mkdir /user/hive/warehouse

-- set read/write permission
hadoop fs -chmod g+w /user/hive/warehouse

-- start
hive




export JAVA_HOME=/opt/obuildfactory/jdk-1.8.0-openjdk-x86_64
export HADOOP_HOME=/usr/local/hadoop
export HIVE_HOME="/usr/local/hive"

export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin

