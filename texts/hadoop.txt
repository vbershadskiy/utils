http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html

$ sudo addgroup hadoop
$ sudo adduser --ingroup hadoop hduser

-- ssh access
$ su - hduser

-- generate an SSH key for the hduser user
$ ssh-keygen -t rsa -P ""

-- enable SSH access to your local machine with this newly created key
$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

-- test
ssh localhost

-- download
hadoop-2.6.0.tar.gz

--hadoop start/restart
/usr/local/hadoop/sbin/start-dfs.sh

--hadoop stop
/usr/local/hadoop/sbin/stop-dfs.sh

--start yarn
/usr/local/hadoop/sbin/start-yarn.sh

--stop yarn
/usr/local/hadoop/sbin/stop-yarn.sh

-- create dir
hadoop fs -mkdir /user
hadoop fs -mkdir /user/hduser
hadoop fs -mkdir /user/hduser/gutenberg

-- get book
cd /tmp/gutenberg
wget https://www.gutenberg.org/ebooks/35997.txt.utf-8

-- copy book to hdfs
hdfs dfs -copyFromLocal /tmp/gutenberg /user/hduser/gutenberg

-- check contents of directory
hdfs dfs -ls /user/hduser/gutenberg

-- remove if exists already
hadoop fs -rm -r /user/hduser/gutenberg-output

-- run example wordcount map reduce job
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /user/hduser/gutenberg/* /user/hduser/gutenberg-output

-- check output was generated
hdfs dfs -ls /user/hduser/gutenberg-output

-- copy output to local directory
hadoop fs -copyToLocal /user/hduser/gutenberg-output/part-r-00000 /home/hduser/projects

-- check hdfs webui 
http://localhost:50070/ – web UI of the NameNode daemon
http://localhost:50030/ – web UI of the JobTracker daemon
http://localhost:50060/ – web UI of the TaskTracker daemon





-- set environment variables
export JAVA_HOME=/opt/obuildfactory/jdk-1.8.0-openjdk-x86_64
export HADOOP_HOME=/usr/local/hadoop
export HIVE_HOME=/usr/local/hive
export HIVE_AUX_JARS_PATH=/usr/local/hive/lib
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin





-- install hive
hadoop fs -mkdir /user/hive/warehouse
hadoop fs -mkdir /tmp

-- set read/write permission
hadoop fs -chmod g+w /user/hive/warehouse
hadoop fs -chmod g+w /tmp

-- start
hive

-- create dual
create table dual (dummy string);
load data inpath '/tmp/dual.txt' overwrite into table dual;

-- create seed.txt file on hdfs
http://www.bla.com/path&term=bla1	1	a
http://www.bla.com/path&term=bla2	2	b
http://www.bla.com/path&term=bla3	3	c

hdfs dfs -copyFromLocal ./urls /

-- create hive table from existing hdfs file (when you drop an external table, it only drops the metadata)
create external table urls (
  url STRING,
  id1 STRING,
  id2 STRING
) row format delimited
fields terminated by '\t'
stored as textfile
location '/urls';

-- replace contents of table
insert into table urls values('http://www.bla.com/path&term=bla4', 4, 'd');

insert overwrite directory '/urls' select * from dual;


